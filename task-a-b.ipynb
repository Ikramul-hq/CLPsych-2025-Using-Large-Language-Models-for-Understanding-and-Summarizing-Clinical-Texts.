{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10964769,"sourceType":"datasetVersion","datasetId":6821926},{"sourceId":10964772,"sourceType":"datasetVersion","datasetId":6821929}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport os\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\ndef load_all_timelines(data_dir):\n    \"\"\"Load all JSON files from a directory into a list of timelines\"\"\"\n    timelines = []\n    for filename in os.listdir(data_dir):\n        if filename.endswith('.json'):\n            with open(os.path.join(data_dir, filename), 'r', encoding='utf-8') as f:\n                timeline = json.load(f)\n                timelines.append(timeline)\n    return timelines\n\ndef create_training_dataset(timelines):\n    \"\"\"Extract posts with their evidence annotations into a DataFrame\"\"\"\n    data = []\n    for timeline in timelines:\n        timeline_id = timeline[\"timeline_id\"]\n        for post in timeline[\"posts\"]:\n            post_id = post[\"post_id\"]\n            post_text = post[\"post\"]\n            \n            # Skip posts without evidence annotations\n            if \"evidence\" not in post:\n                continue\n                \n            # Extract adaptive evidence spans\n            adaptive_evidence = []\n            if \"adaptive-state\" in post[\"evidence\"]:\n                for component, details in post[\"evidence\"][\"adaptive-state\"].items():\n                    if \"highlighted_evidence\" in details:\n                        adaptive_evidence.append(details[\"highlighted_evidence\"])\n            \n            # Extract maladaptive evidence spans\n            maladaptive_evidence = []\n            if \"maladaptive-state\" in post[\"evidence\"]:\n                for component, details in post[\"evidence\"][\"maladaptive-state\"].items():\n                    if \"highlighted_evidence\" in details:\n                        maladaptive_evidence.append(details[\"highlighted_evidence\"])\n            \n            data.append({\n                \"timeline_id\": timeline_id,\n                \"post_id\": post_id,\n                \"text\": post_text,\n                \"adaptive_evidence\": adaptive_evidence,\n                \"maladaptive_evidence\": maladaptive_evidence\n            })\n    \n    return pd.DataFrame(data)\n\n# Load all training timelines\ntrain_timelines = load_all_timelines(\"/kaggle/input/train-dataset-1\")\ntrain_df = create_training_dataset(train_timelines)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T08:18:09.681153Z","iopub.execute_input":"2025-03-09T08:18:09.681443Z","iopub.status.idle":"2025-03-09T08:18:09.709593Z","shell.execute_reply.started":"2025-03-09T08:18:09.681422Z","shell.execute_reply":"2025-03-09T08:18:09.708923Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\n\ndef analyze_dataset(df):\n    \"\"\"Analyze the training dataset to understand its characteristics\"\"\"\n    print(f\"Total posts: {len(df)}\")\n    print(f\"Posts with adaptive evidence: {sum(df['adaptive_evidence'].apply(len) > 0)}\")\n    print(f\"Posts with maladaptive evidence: {sum(df['maladaptive_evidence'].apply(len) > 0)}\")\n    \n    # Create binary labels for classification\n    df['has_adaptive'] = df['adaptive_evidence'].apply(lambda x: 1 if len(x) > 0 else 0)\n    df['has_maladaptive'] = df['maladaptive_evidence'].apply(lambda x: 1 if len(x) > 0 else 0)\n    \n    # Tokenize posts into sentences for later use\n    df['sentences'] = df['text'].apply(sent_tokenize)\n    \n    return df\n\n# Analyze and preprocess the dataset\ntrain_df = analyze_dataset(train_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T08:18:09.710541Z","iopub.execute_input":"2025-03-09T08:18:09.710728Z","iopub.status.idle":"2025-03-09T08:18:09.784392Z","shell.execute_reply.started":"2025-03-09T08:18:09.710711Z","shell.execute_reply":"2025-03-09T08:18:09.783754Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nTotal posts: 343\nPosts with adaptive evidence: 169\nPosts with maladaptive evidence: 179\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\ndef engineer_features(df, feature_type=\"tfidf\"):\n    \"\"\"Create features for binary classification\"\"\"\n    if feature_type == \"tfidf\":\n        # TF-IDF features\n        vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n        X = vectorizer.fit_transform(df['text'])\n        feature_names = vectorizer.get_feature_names_out()\n    else:\n        # Add other feature types if needed (e.g., BERT embeddings)\n        pass\n    \n    return X, vectorizer, feature_names\n\n# Split data into train and validation sets\ntrain_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)\n\n# Create feature matrices\nX_train, vectorizer, feature_names = engineer_features(train_data)\nX_val = vectorizer.transform(val_data['text'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T08:18:09.785688Z","iopub.execute_input":"2025-03-09T08:18:09.785885Z","iopub.status.idle":"2025-03-09T08:18:09.878600Z","shell.execute_reply.started":"2025-03-09T08:18:09.785868Z","shell.execute_reply":"2025-03-09T08:18:09.877969Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\ndef train_binary_classifiers(X_train, y_train_adaptive, y_train_maladaptive, \n                             X_val, y_val_adaptive, y_val_maladaptive):\n    \"\"\"Train separate classifiers for adaptive and maladaptive states\"\"\"\n    # Adaptive classifier\n    adaptive_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n    adaptive_clf.fit(X_train, y_train_adaptive)\n    adaptive_preds = adaptive_clf.predict(X_val)\n    \n    # Maladaptive classifier\n    maladaptive_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n    maladaptive_clf.fit(X_train, y_train_maladaptive)\n    maladaptive_preds = maladaptive_clf.predict(X_val)\n    \n    # Evaluate classifiers\n    print(\"Adaptive Classifier Performance:\")\n    print(classification_report(y_val_adaptive, adaptive_preds))\n    \n    print(\"Maladaptive Classifier Performance:\")\n    print(classification_report(y_val_maladaptive, maladaptive_preds))\n    \n    return adaptive_clf, maladaptive_clf\n\n# Train classifiers\nadaptive_clf, maladaptive_clf = train_binary_classifiers(\n    X_train, train_data['has_adaptive'], train_data['has_maladaptive'],\n    X_val, val_data['has_adaptive'], val_data['has_maladaptive']\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T08:18:27.280519Z","iopub.execute_input":"2025-03-09T08:18:27.280802Z","iopub.status.idle":"2025-03-09T08:18:27.735671Z","shell.execute_reply.started":"2025-03-09T08:18:27.280780Z","shell.execute_reply":"2025-03-09T08:18:27.734939Z"}},"outputs":[{"name":"stdout","text":"Adaptive Classifier Performance:\n              precision    recall  f1-score   support\n\n           0       0.79      0.81      0.80        37\n           1       0.77      0.75      0.76        32\n\n    accuracy                           0.78        69\n   macro avg       0.78      0.78      0.78        69\nweighted avg       0.78      0.78      0.78        69\n\nMaladaptive Classifier Performance:\n              precision    recall  f1-score   support\n\n           0       0.94      0.83      0.88        41\n           1       0.79      0.93      0.85        28\n\n    accuracy                           0.87        69\n   macro avg       0.87      0.88      0.87        69\nweighted avg       0.88      0.87      0.87        69\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"def calculate_feature_importance_simple(clf, feature_names):\n    \"\"\"Calculate feature importance using the classifier's built-in feature_importances_\"\"\"\n    if not hasattr(clf, 'feature_importances_'):\n        raise ValueError(\"Classifier does not have feature_importances_ attribute\")\n    \n    importance_scores = clf.feature_importances_\n    \n    # Create a mapping of features to importance scores\n    feature_importance = {feature_names[i]: importance_scores[i] \n                         for i in range(len(feature_names))}\n    \n    return feature_importance\n\n# Calculate feature importance for adaptive classifier\nadaptive_importance = calculate_feature_importance_simple(adaptive_clf, feature_names)\nprint(f\"Top 5 important features for adaptive states: {sorted(adaptive_importance.items(), key=lambda x: x[1], reverse=True)[:5]}\")\n\n# Calculate feature importance for maladaptive classifier\nmaladaptive_importance = calculate_feature_importance_simple(maladaptive_clf, feature_names)\nprint(f\"Top 5 important features for maladaptive states: {sorted(maladaptive_importance.items(), key=lambda x: x[1], reverse=True)[:5]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T08:18:30.951275Z","iopub.execute_input":"2025-03-09T08:18:30.951580Z","iopub.status.idle":"2025-03-09T08:18:31.011255Z","shell.execute_reply.started":"2025-03-09T08:18:30.951559Z","shell.execute_reply":"2025-03-09T08:18:31.010553Z"},"_kg_hide-output":false},"outputs":[{"name":"stdout","text":"Top 5 important features for adaptive states: [('and', 0.022689654908040886), ('but', 0.021444647353222553), ('it', 0.018397119002538157), ('to', 0.016057036870414175), ('for', 0.014616723314670446)]\nTop 5 important features for maladaptive states: [('and', 0.02563150227239303), ('to', 0.02274935575225972), ('it', 0.018220202868338312), ('be', 0.016732570023576965), ('of', 0.01626467195205366)]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import numpy as np\nfrom scipy import sparse\n\ndef extract_evidence_spans(post_text, clf, vectorizer, feature_names, \n                           importance_threshold=0.01, top_n=3, feature_importance=None):\n    \"\"\"\n    Extract evidence spans for a post using feature importance\n    \n    Parameters:\n    -----------\n    post_text : str\n        The text of the post to analyze\n    clf : classifier\n        Trained classifier model\n    vectorizer : TfidfVectorizer or similar\n        The vectorizer used to convert text to features\n    feature_names : list\n        Names of features\n    importance_threshold : float\n        Threshold for considering a feature important\n    top_n : int\n        Number of top sentences to return\n    feature_importance : dict, optional\n        Pre-calculated feature importance dictionary\n    \"\"\"\n    # Vectorize the post\n    post_vector = vectorizer.transform([post_text])\n    \n    # Predict if post contains evidence\n    try:\n        has_evidence = clf.predict(post_vector)[0]\n    except Exception as e:\n        print(f\"Prediction error: {str(e)}\")\n        return []\n    \n    if not has_evidence:\n        return []\n    \n    # Use pre-calculated feature importance if provided\n    if feature_importance is not None:\n        post_importance = feature_importance\n    else:\n        try:\n            # Use classifier's built-in feature importance instead of SHAP\n            # This avoids the sparse matrix issues with SHAP\n            if hasattr(clf, 'feature_importances_'):\n                importance_scores = clf.feature_importances_\n                post_importance = {feature_names[i]: importance_scores[i] \n                                  for i in range(len(feature_names))}\n            else:\n                # If no feature importance available, use coefficient values for linear models\n                if hasattr(clf, 'coef_'):\n                    coef = clf.coef_[0] if len(clf.coef_.shape) > 1 else clf.coef_\n                    post_importance = {feature_names[i]: abs(coef[i]) \n                                      for i in range(len(feature_names))}\n                else:\n                    # Last resort: give equal importance to all features present in this post\n                    post_importance = {}\n                    post_vector_array = post_vector.toarray()[0]\n                    for i, val in enumerate(post_vector_array):\n                        if val > 0:\n                            post_importance[feature_names[i]] = val\n        except Exception as e:\n            print(f\"Feature importance calculation error: {str(e)}\")\n            # Create a simple importance score based on TF-IDF values\n            post_importance = {}\n            for i, val in enumerate(post_vector.toarray()[0]):\n                if val > 0:\n                    post_importance[feature_names[i]] = val\n    \n    # Find important sentences\n    important_sentences = find_important_sentences(\n        post_text, post_importance, importance_threshold\n    )\n    \n    # Return top N sentences as evidence spans\n    spans = [s['sentence'] for s in important_sentences[:top_n] if s['score'] > 0]\n    \n    # Ensure spans are actually in the original text\n    validated_spans = [span for span in spans if span in post_text]\n    \n    return validated_spans\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T08:18:36.311171Z","iopub.execute_input":"2025-03-09T08:18:36.311450Z","iopub.status.idle":"2025-03-09T08:18:36.319520Z","shell.execute_reply.started":"2025-03-09T08:18:36.311430Z","shell.execute_reply":"2025-03-09T08:18:36.318564Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def consolidate_spans(spans, post_text):\n    \"\"\"Merge overlapping spans and ensure they are continuous in the original text\"\"\"\n    if not spans:\n        return []\n    \n    # Sort spans by their position in the original text\n    sorted_spans = sorted(spans, key=lambda span: post_text.find(span))\n    \n    # Merge spans that are adjacent or overlapping in the original text\n    consolidated = []\n    current_start = post_text.find(sorted_spans[0])\n    current_end = current_start + len(sorted_spans[0])\n    current_span = sorted_spans[0]\n    \n    for span in sorted_spans[1:]:\n        span_start = post_text.find(span)\n        span_end = span_start + len(span)\n        \n        # If spans overlap or are adjacent, merge them\n        if span_start <= current_end + 5:  # Allow small gaps (5 chars)\n            merged_end = max(current_end, span_end)\n            current_span = post_text[current_start:merged_end]\n            current_end = merged_end\n        else:\n            consolidated.append(current_span)\n            current_start = span_start\n            current_end = span_end\n            current_span = span\n    \n    consolidated.append(current_span)\n    return consolidated\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T08:18:40.496197Z","iopub.execute_input":"2025-03-09T08:18:40.496479Z","iopub.status.idle":"2025-03-09T08:18:40.501868Z","shell.execute_reply.started":"2025-03-09T08:18:40.496459Z","shell.execute_reply":"2025-03-09T08:18:40.501103Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def process_test_timeline(timeline, adaptive_clf, maladaptive_clf, vectorizer, feature_names, \n                         adaptive_importance=None, maladaptive_importance=None):\n    \"\"\"Process a test timeline to extract evidence spans for each post\"\"\"\n    timeline_id = timeline[\"timeline_id\"]\n    result = {\n        \"timeline_level\": {\"summary\": \"\"},  # Will be filled by Task C\n        \"post_level\": {}\n    }\n    \n    for post in timeline[\"posts\"]:\n        post_id = post[\"post_id\"]\n        post_text = post[\"post\"] if \"post\" in post else \"\"\n        \n        if not post_text:\n            # Handle empty posts\n            result[\"post_level\"][post_id] = {\n                \"adaptive_evidence\": [],\n                \"maladaptive_evidence\": [],\n                \"summary\": \"\",\n                \"wellbeing_score\": None\n            }\n            continue\n            \n        # Extract adaptive evidence\n        adaptive_spans = extract_evidence_spans(\n            post_text, adaptive_clf, vectorizer, feature_names,\n            feature_importance=adaptive_importance\n        )\n        adaptive_spans = consolidate_spans(adaptive_spans, post_text)\n        \n        # Extract maladaptive evidence\n        maladaptive_spans = extract_evidence_spans(\n            post_text, maladaptive_clf, vectorizer, feature_names,\n            feature_importance=maladaptive_importance\n        )\n        maladaptive_spans = consolidate_spans(maladaptive_spans, post_text)\n        \n        # Add to results\n        result[\"post_level\"][post_id] = {\n            \"adaptive_evidence\": adaptive_spans,\n            \"maladaptive_evidence\": maladaptive_spans,\n            \"summary\": \"\",  # Will be filled by Task B\n            \"wellbeing_score\": None  # Will be filled by Task A.2\n        }\n    \n    return timeline_id, result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T08:18:43.672690Z","iopub.execute_input":"2025-03-09T08:18:43.673136Z","iopub.status.idle":"2025-03-09T08:18:43.680589Z","shell.execute_reply.started":"2025-03-09T08:18:43.673108Z","shell.execute_reply":"2025-03-09T08:18:43.679626Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def calculate_feature_importance_simple(clf, feature_names):\n    \"\"\"Calculate feature importance using the classifier's built-in feature_importances_\"\"\"\n    if hasattr(clf, 'feature_importances_'):\n        importance_scores = clf.feature_importances_\n    elif hasattr(clf, 'coef_'):\n        coef = clf.coef_[0] if len(clf.coef_.shape) > 1 else clf.coef_\n        importance_scores = np.abs(coef)\n    else:\n        raise ValueError(\"Classifier does not have feature_importances_ or coef_ attribute\")\n    \n    # Create a mapping of features to importance scores\n    feature_importance = {feature_names[i]: float(importance_scores[i]) \n                         for i in range(len(feature_names))}\n    \n    return feature_importance\n\ndef run_full_pipeline(train_dir, test_dir, output_path, team_name=\"MyTeam\"):\n    \"\"\"Run the complete Task A.1 pipeline from training to submission generation\"\"\"\n    print(\"Loading training data...\")\n    train_timelines = load_all_timelines(train_dir)\n    train_df = create_training_dataset(train_timelines)\n    train_df = analyze_dataset(train_df)\n    \n    print(\"Training classifiers...\")\n    train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)\n    X_train, vectorizer, feature_names = engineer_features(train_data)\n    X_val = vectorizer.transform(val_data['text'])\n    \n    adaptive_clf, maladaptive_clf = train_binary_classifiers(\n        X_train, train_data['has_adaptive'], train_data['has_maladaptive'],\n        X_val, val_data['has_adaptive'], val_data['has_maladaptive']\n    )\n    \n    print(\"Calculating feature importance...\")\n    # Calculate feature importance using the simplified method\n    try:\n        adaptive_importance = calculate_feature_importance_simple(adaptive_clf, feature_names)\n        print(\"Feature importance for adaptive classifier calculated successfully\")\n    except Exception as e:\n        print(f\"Error calculating importance for adaptive classifier: {str(e)}\")\n        adaptive_importance = None\n        \n    try:\n        maladaptive_importance = calculate_feature_importance_simple(maladaptive_clf, feature_names)\n        print(\"Feature importance for maladaptive classifier calculated successfully\")\n    except Exception as e:\n        print(f\"Error calculating importance for maladaptive classifier: {str(e)}\")\n        maladaptive_importance = None\n    \n    print(\"Processing test data...\")\n    test_timelines = load_all_timelines(test_dir)\n    \n    submission = {}\n    for timeline in tqdm(test_timelines, desc=\"Processing test timelines\"):\n        timeline_id, result = process_test_timeline(\n            timeline, adaptive_clf, maladaptive_clf, vectorizer, feature_names,\n            adaptive_importance, maladaptive_importance\n        )\n        submission[timeline_id] = result\n    \n    print(\"Saving submission...\")\n    os.makedirs(output_path, exist_ok=True)\n    output_file = os.path.join(output_path, f\"{team_name}_1.json\")\n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(submission, f, ensure_ascii=False, indent=2)\n    \n    print(f\"Task A.1 processing complete! Submission saved to {output_file}\")\n    return submission\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T08:18:46.768595Z","iopub.execute_input":"2025-03-09T08:18:46.768876Z","iopub.status.idle":"2025-03-09T08:18:46.777497Z","shell.execute_reply.started":"2025-03-09T08:18:46.768855Z","shell.execute_reply":"2025-03-09T08:18:46.776670Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def evaluate_evidence_extraction(val_data, predictions, metric=\"bertscore\"):\n    \"\"\"Evaluate evidence extraction performance using BERTScore\"\"\"\n    from bert_score import score\n    \n    adaptive_scores = []\n    maladaptive_scores = []\n    \n    for i, row in val_data.iterrows():\n        post_id = row['post_id']\n        if post_id in predictions:\n            # Evaluate adaptive evidence\n            gold_adaptive = row['adaptive_evidence']\n            pred_adaptive = predictions[post_id]['adaptive_evidence']\n            \n            if gold_adaptive and pred_adaptive:\n                P, R, F1 = score(pred_adaptive, gold_adaptive, lang=\"en\")\n                adaptive_scores.append(R.mean().item())  # Use recall as per CLPsych evaluation\n            \n            # Evaluate maladaptive evidence\n            gold_maladaptive = row['maladaptive_evidence']\n            pred_maladaptive = predictions[post_id]['maladaptive_evidence']\n            \n            if gold_maladaptive and pred_maladaptive:\n                P, R, F1 = score(pred_maladaptive, gold_maladaptive, lang=\"en\")\n                maladaptive_scores.append(R.mean().item())\n    \n    return {\n        \"adaptive_recall\": np.mean(adaptive_scores) if adaptive_scores else 0,\n        \"maladaptive_recall\": np.mean(maladaptive_scores) if maladaptive_scores else 0,\n        \"overall_recall\": np.mean(adaptive_scores + maladaptive_scores) if adaptive_scores + maladaptive_scores else 0\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T08:18:51.983126Z","iopub.execute_input":"2025-03-09T08:18:51.983429Z","iopub.status.idle":"2025-03-09T08:18:51.989243Z","shell.execute_reply.started":"2025-03-09T08:18:51.983404Z","shell.execute_reply":"2025-03-09T08:18:51.988321Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ndef tune_classifiers(X_train, y_train):\n    \"\"\"Find optimal hyperparameters for the classifiers\"\"\"\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'max_depth': [None, 10, 20, 30],\n        'min_samples_split': [2, 5, 10]\n    }\n    \n    grid_search = GridSearchCV(\n        RandomForestClassifier(random_state=42),\n        param_grid,\n        cv=5,\n        scoring='f1',\n        n_jobs=-1\n    )\n    \n    grid_search.fit(X_train, y_train)\n    return grid_search.best_estimator_, grid_search.best_params_\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T08:18:54.982552Z","iopub.execute_input":"2025-03-09T08:18:54.982856Z","iopub.status.idle":"2025-03-09T08:18:54.987445Z","shell.execute_reply.started":"2025-03-09T08:18:54.982830Z","shell.execute_reply":"2025-03-09T08:18:54.986709Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def extract_bert_features(texts, model_name=\"emilyalsentzer/Bio_ClinicalBERT\"):\n    \"\"\"Extract BERT embeddings as features\"\"\"\n    from transformers import AutoTokenizer, AutoModel\n    import torch\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModel.from_pretrained(model_name)\n    \n    embeddings = []\n    for text in tqdm(texts, desc=\"Extracting BERT embeddings\"):\n        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n        with torch.no_grad():\n            outputs = model(**inputs)\n        \n        # Use CLS token embedding as document representation\n        embeddings.append(outputs.last_hidden_state[:, 0, :].numpy().flatten())\n    \n    return np.array(embeddings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T08:19:00.575899Z","iopub.execute_input":"2025-03-09T08:19:00.576245Z","iopub.status.idle":"2025-03-09T08:19:00.581622Z","shell.execute_reply.started":"2025-03-09T08:19:00.576215Z","shell.execute_reply":"2025-03-09T08:19:00.580562Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def generate_final_submission(test_dir, output_dir, team_name, model_params):\n    \"\"\"Generate the final submission file for Task A.1\"\"\"\n    # Load trained models and vectorizers\n    adaptive_clf = model_params['adaptive_clf']\n    maladaptive_clf = model_params['maladaptive_clf']\n    vectorizer = model_params['vectorizer']\n    feature_names = model_params['feature_names']\n    \n    # Process test timelines\n    test_timelines = load_all_timelines(test_dir)\n    submission = {}\n    \n    for timeline in tqdm(test_timelines, desc=\"Generating final predictions\"):\n        timeline_id, result = process_test_timeline(\n            timeline, adaptive_clf, maladaptive_clf, vectorizer, feature_names\n        )\n        submission[timeline_id] = result\n    \n    # Save submission file\n    output_file = os.path.join(output_dir, f\"{team_name}_TaskA1.json\")\n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(submission, f, ensure_ascii=False, indent=2)\n    \n    print(f\"Final submission saved to {output_file}\")\n    return submission\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T08:19:03.407874Z","iopub.execute_input":"2025-03-09T08:19:03.408194Z","iopub.status.idle":"2025-03-09T08:19:03.413345Z","shell.execute_reply.started":"2025-03-09T08:19:03.408168Z","shell.execute_reply":"2025-03-09T08:19:03.412550Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Configuration\n    TRAIN_DIR = \"/kaggle/input/train-dataset-1\"\n    TEST_DIR = \"/kaggle/input/test-dataset-1\"\n    OUTPUT_DIR = \"/kaggle/working/\"\n    TEAM_NAME = \"CIOL\"\n    \n    # Run the complete pipeline\n    submission = run_full_pipeline(TRAIN_DIR, TEST_DIR, OUTPUT_DIR, TEAM_NAME)\n    \n    print(\"Task A.1 completed successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T08:19:06.358615Z","iopub.execute_input":"2025-03-09T08:19:06.358920Z","iopub.status.idle":"2025-03-09T08:19:07.066833Z","shell.execute_reply.started":"2025-03-09T08:19:06.358892Z","shell.execute_reply":"2025-03-09T08:19:07.065693Z"}},"outputs":[{"name":"stdout","text":"Loading training data...\nTotal posts: 343\nPosts with adaptive evidence: 169\nPosts with maladaptive evidence: 179\nTraining classifiers...\nAdaptive Classifier Performance:\n              precision    recall  f1-score   support\n\n           0       0.79      0.81      0.80        37\n           1       0.77      0.75      0.76        32\n\n    accuracy                           0.78        69\n   macro avg       0.78      0.78      0.78        69\nweighted avg       0.78      0.78      0.78        69\n\nMaladaptive Classifier Performance:\n              precision    recall  f1-score   support\n\n           0       0.94      0.83      0.88        41\n           1       0.79      0.93      0.85        28\n\n    accuracy                           0.87        69\n   macro avg       0.87      0.88      0.87        69\nweighted avg       0.88      0.87      0.87        69\n\nCalculating feature importance...\nFeature importance for adaptive classifier calculated successfully\nFeature importance for maladaptive classifier calculated successfully\nProcessing test data...\n","output_type":"stream"},{"name":"stderr","text":"Processing test timelines:   0%|          | 0/10 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-b7fd8bde8111>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Run the complete pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_full_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEAM_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Task A.1 completed successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-7534c0ca1c0d>\u001b[0m in \u001b[0;36mrun_full_pipeline\u001b[0;34m(train_dir, test_dir, output_path, team_name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtimeline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_timelines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Processing test timelines\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         timeline_id, result = process_test_timeline(\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mtimeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madaptive_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaladaptive_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0madaptive_importance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaladaptive_importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-05475f884f87>\u001b[0m in \u001b[0;36mprocess_test_timeline\u001b[0;34m(timeline, adaptive_clf, maladaptive_clf, vectorizer, feature_names, adaptive_importance, maladaptive_importance)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Extract adaptive evidence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         adaptive_spans = extract_evidence_spans(\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mpost_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madaptive_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mfeature_importance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madaptive_importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-27484154656b>\u001b[0m in \u001b[0;36mextract_evidence_spans\u001b[0;34m(post_text, clf, vectorizer, feature_names, importance_threshold, top_n, feature_importance)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# Find important sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     important_sentences = find_important_sentences(\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mpost_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_importance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimportance_threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     )\n","\u001b[0;31mNameError\u001b[0m: name 'find_important_sentences' is not defined"],"ename":"NameError","evalue":"name 'find_important_sentences' is not defined","output_type":"error"}],"execution_count":28},{"cell_type":"code","source":"import json\nimport os\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef extract_wellbeing_data(timelines):\n    \"\"\"Extract posts with well-being scores from training timelines\"\"\"\n    data = []\n    for timeline in timelines:\n        timeline_id = timeline[\"timeline_id\"]\n        prev_posts = []  # To store previous posts for contextual features\n        \n        for i, post in enumerate(timeline[\"posts\"]):\n            post_id = post[\"post_id\"]\n            post_text = post[\"post\"]\n            \n            # Skip posts without well-being annotations\n            if \"Well-being\" not in post or post[\"Well-being\"] is None:\n                prev_posts.append(post_text)\n                continue\n                \n            wellbeing_score = post[\"Well-being\"]\n            \n            # Extract adaptive and maladaptive evidence\n            adaptive_evidence = []\n            maladaptive_evidence = []\n            if \"evidence\" in post:\n                if \"adaptive-state\" in post[\"evidence\"]:\n                    for component, details in post[\"evidence\"][\"adaptive-state\"].items():\n                        if \"highlighted_evidence\" in details:\n                            adaptive_evidence.append(details[\"highlighted_evidence\"])\n                \n                if \"maladaptive-state\" in post[\"evidence\"]:\n                    for component, details in post[\"evidence\"][\"maladaptive-state\"].items():\n                        if \"highlighted_evidence\" in details:\n                            maladaptive_evidence.append(details[\"highlighted_evidence\"])\n            \n            # Get previous posts context (last 3 posts)\n            context = prev_posts[-3:] if prev_posts else []\n            \n            data.append({\n                \"timeline_id\": timeline_id,\n                \"post_id\": post_id,\n                \"post_index\": i,\n                \"text\": post_text,\n                \"adaptive_evidence\": adaptive_evidence,\n                \"maladaptive_evidence\": maladaptive_evidence,\n                \"previous_posts\": context,\n                \"wellbeing_score\": wellbeing_score\n            })\n            \n            # Update previous posts\n            prev_posts.append(post_text)\n    \n    return pd.DataFrame(data)\n\n# Load all training timelines\ndef load_all_timelines(data_dir):\n    \"\"\"Load all JSON files from a directory into a list of timelines\"\"\"\n    timelines = []\n    for filename in os.listdir(data_dir):\n        if filename.endswith('.json'):\n            with open(os.path.join(data_dir, filename), 'r', encoding='utf-8') as f:\n                timeline = json.load(f)\n                timelines.append(timeline)\n    return timelines\n\n# Load and prepare data\ntrain_timelines = load_all_timelines(\"/kaggle/input/train-dataset-1\")\nwellbeing_df = extract_wellbeing_data(train_timelines)\n\n# Analyze well-being score distribution\nscore_distribution = wellbeing_df['wellbeing_score'].value_counts().sort_index()\nprint(\"Well-being score distribution:\")\nprint(score_distribution)\n\n# Split data into training and validation sets\ntrain_data, val_data = train_test_split(wellbeing_df, test_size=0.2, random_state=42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Download required NLTK resources\nnltk.download('vader_lexicon')\nnltk.download('punkt')\n\ndef extract_wellbeing_features(df):\n    \"\"\"\n    Extract features relevant to well-being prediction\n    \n    Features include:\n    - Text features (length, sentiment)\n    - Evidence-based features (presence, count, ratio)\n    - Previous post context features\n    - Content-based features (mentions of specific topics)\n    \"\"\"\n    # Initialize sentiment analyzer\n    sid = SentimentIntensityAnalyzer()\n    \n    # Create feature DataFrame\n    features = pd.DataFrame()\n    \n    # 1. Basic text features\n    features['text_length'] = df['text'].apply(len)\n    features['word_count'] = df['text'].apply(lambda x: len(x.split()))\n    features['sent_count'] = df['text'].apply(lambda x: len(nltk.sent_tokenize(x)))\n    \n    # 2. Sentiment features\n    features['sentiment_neg'] = df['text'].apply(lambda x: sid.polarity_scores(x)['neg'])\n    features['sentiment_neu'] = df['text'].apply(lambda x: sid.polarity_scores(x)['neu'])\n    features['sentiment_pos'] = df['text'].apply(lambda x: sid.polarity_scores(x)['pos'])\n    features['sentiment_compound'] = df['text'].apply(lambda x: sid.polarity_scores(x)['compound'])\n    \n    # 3. Evidence-based features\n    features['has_adaptive'] = df['adaptive_evidence'].apply(lambda x: 1 if len(x) > 0 else 0)\n    features['has_maladaptive'] = df['maladaptive_evidence'].apply(lambda x: 1 if len(x) > 0 else 0)\n    features['adaptive_count'] = df['adaptive_evidence'].apply(len)\n    features['maladaptive_count'] = df['maladaptive_evidence'].apply(len)\n    \n    # Calculate ratio of adaptive to total evidence spans\n    features['adaptive_ratio'] = features.apply(\n        lambda row: row['adaptive_count'] / (row['adaptive_count'] + row['maladaptive_count']) \n        if (row['adaptive_count'] + row['maladaptive_count']) > 0 else 0.5,\n        axis=1\n    )\n    \n    # 4. Context features (if previous posts exist)\n    features['has_prev_posts'] = df['previous_posts'].apply(lambda x: 1 if len(x) > 0 else 0)\n    features['prev_posts_count'] = df['previous_posts'].apply(len)\n    \n    # 5. Content-based features\n    # Detect mentions of specific topics related to well-being\n    # Social functioning\n    features['mentions_friends'] = df['text'].apply(\n        lambda x: 1 if re.search(r'\\b(friend|friends|social|relationship|relationships)\\b', x.lower()) else 0\n    )\n    features['mentions_family'] = df['text'].apply(\n        lambda x: 1 if re.search(r'\\b(family|parent|parents|mom|dad|sister|brother|sibling)\\b', x.lower()) else 0\n    )\n    \n    # Occupational functioning\n    features['mentions_work'] = df['text'].apply(\n        lambda x: 1 if re.search(r'\\b(work|job|career|school|college|university|study|studies)\\b', x.lower()) else 0\n    )\n    \n    # Psychological functioning\n    features['mentions_mental_health'] = df['text'].apply(\n        lambda x: 1 if re.search(r'\\b(depress|anxiety|stress|mental|therapy|therapist|psychologist|psychiatrist)\\b', \n                                x.lower()) else 0\n    )\n    features['mentions_suicide'] = df['text'].apply(\n        lambda x: 1 if re.search(r'\\b(suicid|kill myself|end my life|die|death)\\b', x.lower()) else 0\n    )\n    features['mentions_self_harm'] = df['text'].apply(\n        lambda x: 1 if re.search(r'\\b(cut|cutting|self-harm|hurt myself|harm myself)\\b', x.lower()) else 0\n    )\n    \n    return features\n\n# Extract features from training and validation data\ntrain_features = extract_wellbeing_features(train_data)\nval_features = extract_wellbeing_features(val_data)\n\n# Add text vectorization features\nvectorizer = TfidfVectorizer(max_features=500, stop_words='english')\ntrain_tfidf = vectorizer.fit_transform(train_data['text'])\nval_tfidf = vectorizer.transform(val_data['text'])\n\n# Convert sparse matrices to DataFrames\ntrain_tfidf_df = pd.DataFrame(\n    train_tfidf.toarray(), \n    columns=[f'tfidf_{i}' for i in range(train_tfidf.shape[1])]\n)\nval_tfidf_df = pd.DataFrame(\n    val_tfidf.toarray(), \n    columns=[f'tfidf_{i}' for i in range(val_tfidf.shape[1])]\n)\n\n# Combine all features\ntrain_features_full = pd.concat([train_features, train_tfidf_df], axis=1)\nval_features_full = pd.concat([val_features, val_tfidf_df], axis=1)\n\n# Prepare target values\ntrain_target = train_data['wellbeing_score']\nval_target = val_data['wellbeing_score']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\n\ndef align_features_and_targets(features_df, targets_series):\n    \"\"\"\n    Ensure features and targets are properly aligned with the same number of samples\n    \n    Parameters:\n    -----------\n    features_df : DataFrame\n        Feature dataframe\n    targets_series : Series\n        Target values series\n    \n    Returns:\n    --------\n    tuple: (aligned_features, aligned_targets)\n    \"\"\"\n    print(f\"Original features shape: {features_df.shape}\")\n    print(f\"Original targets shape: {targets_series.shape}\")\n    \n    # Get the intersection of indices\n    common_indices = features_df.index.intersection(targets_series.index)\n    print(f\"Number of common indices: {len(common_indices)}\")\n    \n    # Filter both dataframes to only include common indices\n    aligned_features = features_df.loc[common_indices]\n    aligned_targets = targets_series.loc[common_indices]\n    \n    print(f\"Aligned features shape: {aligned_features.shape}\")\n    print(f\"Aligned targets shape: {aligned_targets.shape}\")\n    \n    # Check for NaN values\n    print(f\"NaN values in aligned features: {aligned_features.isna().sum().sum()}\")\n    print(f\"NaN values in aligned targets: {aligned_targets.isna().sum()}\")\n    \n    return aligned_features, aligned_targets\n\ndef train_wellbeing_models(X_train, y_train, X_val, y_val):\n    \"\"\"Train and evaluate multiple regression models for well-being scoring\"\"\"\n    \n    # First ensure data alignment\n    X_train, y_train = align_features_and_targets(X_train, y_train)\n    X_val, y_val = align_features_and_targets(X_val, y_val)\n    \n    # Handle any remaining NaN values in features\n    imputer = SimpleImputer(strategy='mean')\n    X_train_imputed = imputer.fit_transform(X_train)\n    X_val_imputed = imputer.transform(X_val)\n    \n    # Check for NaN values after imputation\n    print(f\"NaN values in training data after imputation: {np.isnan(X_train_imputed).sum()}\")\n    print(f\"NaN values in validation data after imputation: {np.isnan(X_val_imputed).sum()}\")\n    \n    # Scale features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train_imputed)\n    X_val_scaled = scaler.transform(X_val_imputed)\n    \n    # Model 1: Ridge Regression\n    print(\"Training Ridge Regression model...\")\n    ridge = Ridge(alpha=1.0)\n    ridge.fit(X_train_scaled, y_train)\n    ridge_preds = ridge.predict(X_val_scaled)\n    ridge_mse = mean_squared_error(y_val, ridge_preds)\n    \n    # Round predictions to nearest integer and clip to 1-10 range\n    ridge_preds_rounded = np.round(np.clip(ridge_preds, 1, 10)).astype(int)\n    ridge_mse_rounded = mean_squared_error(y_val, ridge_preds_rounded)\n    \n    print(f\"Ridge Regression MSE: {ridge_mse:.4f}, Rounded MSE: {ridge_mse_rounded:.4f}\")\n    \n    # Model 2: Random Forest\n    print(\"Training Random Forest model...\")\n    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n    rf.fit(X_train_imputed, y_train)\n    rf_preds = rf.predict(X_val_imputed)\n    rf_mse = mean_squared_error(y_val, rf_preds)\n    \n    # Round predictions to nearest integer and clip to 1-10 range\n    rf_preds_rounded = np.round(np.clip(rf_preds, 1, 10)).astype(int)\n    rf_mse_rounded = mean_squared_error(y_val, rf_preds_rounded)\n    \n    print(f\"Random Forest MSE: {rf_mse:.4f}, Rounded MSE: {rf_mse_rounded:.4f}\")\n    \n    # Model 3: Gradient Boosting\n    print(\"Training Gradient Boosting model...\")\n    gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n    gb.fit(X_train_imputed, y_train)\n    gb_preds = gb.predict(X_val_imputed)\n    gb_mse = mean_squared_error(y_val, gb_preds)\n    \n    # Round predictions to nearest integer and clip to 1-10 range\n    gb_preds_rounded = np.round(np.clip(gb_preds, 1, 10)).astype(int)\n    gb_mse_rounded = mean_squared_error(y_val, gb_preds_rounded)\n    \n    print(f\"Gradient Boosting MSE: {gb_mse:.4f}, Rounded MSE: {gb_mse_rounded:.4f}\")\n    \n    # Model 4: XGBoost\n    print(\"Training XGBoost model...\")\n    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n    xgb_model.fit(X_train_imputed, y_train)\n    xgb_preds = xgb_model.predict(X_val_imputed)\n    xgb_mse = mean_squared_error(y_val, xgb_preds)\n    \n    # Round predictions to nearest integer and clip to 1-10 range\n    xgb_preds_rounded = np.round(np.clip(xgb_preds, 1, 10)).astype(int)\n    xgb_mse_rounded = mean_squared_error(y_val, xgb_preds_rounded)\n    \n    print(f\"XGBoost MSE: {xgb_mse:.4f}, Rounded MSE: {xgb_mse_rounded:.4f}\")\n    \n    # Determine best model based on MSE\n    model_mse = {\n        'ridge': ridge_mse_rounded,\n        'rf': rf_mse_rounded,\n        'gb': gb_mse_rounded,\n        'xgb': xgb_mse_rounded\n    }\n    \n    best_model_name = min(model_mse, key=model_mse.get)\n    best_model_dict = {\n        'ridge': ridge,\n        'rf': rf,\n        'gb': gb,\n        'xgb': xgb_model\n    }\n    best_preds_dict = {\n        'ridge': ridge_preds_rounded,\n        'rf': rf_preds_rounded,\n        'gb': gb_preds_rounded,\n        'xgb': xgb_preds_rounded\n    }\n    \n    best_model = best_model_dict[best_model_name]\n    best_preds = best_preds_dict[best_model_name]\n    \n    print(f\"\\nBest model: {best_model_name.upper()} with MSE: {model_mse[best_model_name]:.4f}\")\n    \n    # Evaluate on score ranges\n    def eval_score_range(y_true, y_pred, range_min, range_max):\n        mask = (y_true >= range_min) & (y_true <= range_max)\n        if sum(mask) > 0:\n            return mean_squared_error(y_true[mask], y_pred[mask])\n        return 0\n    \n    low_range_mse = eval_score_range(y_val, best_preds, 1, 4)\n    mid_range_mse = eval_score_range(y_val, best_preds, 5, 6)\n    high_range_mse = eval_score_range(y_val, best_preds, 7, 10)\n    \n    print(f\"MSE for scores 1-4: {low_range_mse:.4f}\")\n    print(f\"MSE for scores 5-6: {mid_range_mse:.4f}\")\n    print(f\"MSE for scores 7-10: {high_range_mse:.4f}\")\n    \n    # Analyze feature importance\n    feature_importance = None\n    if hasattr(best_model, 'feature_importances_'):\n        feature_importance = {\n            column: float(importance) \n            for column, importance in zip(X_train.columns, best_model.feature_importances_)\n        }\n        # Print top 10 important features\n        sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n        print(\"\\nTop 10 important features:\")\n        for feature, importance in sorted_importance[:10]:\n            print(f\"{feature}: {importance:.4f}\")\n    \n    return {\n        'ridge': ridge,\n        'rf': rf,\n        'gb': gb,\n        'xgb': xgb_model,\n        'best_model': best_model,\n        'scaler': scaler,\n        'imputer': imputer,\n        'feature_importance': feature_importance,\n        'X_train_columns': X_train.columns.tolist()  # Store column names for prediction\n    }\n\n# Attempt to train the models with error handling\ntry:\n    print(\"=== Starting well-being model training ===\")\n    # Make sure train_target and val_target are series with indices\n    if not isinstance(train_target, pd.Series):\n        train_target = pd.Series(train_target, index=train_features_full.index)\n    if not isinstance(val_target, pd.Series):\n        val_target = pd.Series(val_target, index=val_features_full.index)\n    \n    # Train models using properly aligned data\n    models = train_wellbeing_models(\n        train_features_full, train_target,\n        val_features_full, val_target\n    )\n    print(\"Model training completed successfully!\")\nexcept Exception as e:\n    print(f\"Error during model training: {str(e)}\")\n    \n    # Create a more basic set of features if the full feature set fails\n    print(\"\\nTrying with a simpler feature set...\")\n    \n    # Extract basic features directly\n    def create_basic_features(texts, wellbeing_scores):\n        data = []\n        for text, score in zip(texts, wellbeing_scores):\n            # Basic text features\n            length = len(text)\n            word_count = len(text.split())\n            \n            # Simple sentiment features (without external libraries)\n            positive_words = ['good', 'great', 'happy', 'joy', 'excellent', 'love', 'positive', 'wonderful']\n            negative_words = ['bad', 'sad', 'angry', 'depressed', 'awful', 'hate', 'negative', 'terrible']\n            \n            pos_count = sum(1 for word in text.lower().split() if word in positive_words)\n            neg_count = sum(1 for word in text.lower().split() if word in negative_words)\n            \n            data.append({\n                'text_length': length,\n                'word_count': word_count,\n                'positive_word_count': pos_count,\n                'negative_word_count': neg_count,\n                'pos_neg_ratio': pos_count / (neg_count + 1),  # +1 to avoid division by zero\n                'wellbeing_score': score\n            })\n        \n        return pd.DataFrame(data)\n    \n    # Create simple features from text\n    train_texts = train_data['text'].tolist()\n    train_scores = train_data['wellbeing_score'].tolist()\n    val_texts = val_data['text'].tolist()\n    val_scores = val_data['wellbeing_score'].tolist()\n    \n    simple_train_df = create_basic_features(train_texts, train_scores)\n    simple_val_df = create_basic_features(val_texts, val_scores)\n    \n    # Split features and target\n    simple_train_features = simple_train_df.drop('wellbeing_score', axis=1)\n    simple_train_target = simple_train_df['wellbeing_score']\n    simple_val_features = simple_val_df.drop('wellbeing_score', axis=1)\n    simple_val_target = simple_val_df['wellbeing_score']\n    \n    # Try training with simple features\n    models = train_wellbeing_models(\n        simple_train_features, simple_train_target,\n        simple_val_features, simple_val_target\n    )\n    print(\"Model training completed with simplified feature set!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\nfrom sklearn.neural_network import MLPRegressor\n\ndef extract_bert_embeddings(texts, model_name=\"emilyalsentzer/Bio_ClinicalBERT\"):\n    \"\"\"\n    Extract BERT embeddings for texts\n    \n    Parameters:\n    texts (list): List of text strings\n    model_name (str): Pretrained model name\n    \n    Returns:\n    numpy.ndarray: BERT embeddings\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModel.from_pretrained(model_name)\n    \n    embeddings = []\n    batch_size = 8  # Process texts in batches\n    \n    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting BERT embeddings\"):\n        batch_texts = texts[i:i+batch_size]\n        \n        # Tokenize and get attention masks\n        encoded = tokenizer(batch_texts, padding=True, truncation=True, \n                           return_tensors=\"pt\", max_length=512)\n        \n        # Extract embeddings\n        with torch.no_grad():\n            outputs = model(**encoded)\n            \n        # Use CLS token embedding as document representation\n        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n        embeddings.extend(batch_embeddings)\n    \n    return np.array(embeddings)\n\ndef train_neural_wellbeing_model(X_train, y_train, X_val, y_val):\n    \"\"\"Train neural network model for well-being prediction\"\"\"\n    \n    # Scale features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    \n    # Neural Network Regressor\n    nn_model = MLPRegressor(\n        hidden_layer_sizes=(100, 50),\n        activation='relu',\n        solver='adam',\n        alpha=0.001,\n        max_iter=500,\n        random_state=42\n    )\n    \n    # Train the model\n    nn_model.fit(X_train_scaled, y_train)\n    \n    # Predict and evaluate\n    nn_preds = nn_model.predict(X_val_scaled)\n    nn_preds_rounded = np.round(np.clip(nn_preds, 1, 10)).astype(int)\n    nn_mse = mean_squared_error(y_val, nn_preds_rounded)\n    \n    print(f\"Neural Network MSE: {nn_mse:.4f}\")\n    \n    return nn_model, scaler\n\n# Extract BERT embeddings for NLP-based well-being prediction\n# Note: This step is optional and can be resource-intensive\ntry:\n    print(\"Extracting BERT embeddings for training data...\")\n    train_bert_embeddings = extract_bert_embeddings(train_data['text'].tolist())\n    val_bert_embeddings = extract_bert_embeddings(val_data['text'].tolist())\n    \n    # Train a neural network using BERT embeddings\n    bert_nn_model, bert_scaler = train_neural_wellbeing_model(\n        train_bert_embeddings, train_target,\n        val_bert_embeddings, val_target\n    )\n    \n    # Add BERT model to the models dictionary\n    models['bert_nn'] = bert_nn_model\n    models['bert_scaler'] = bert_scaler\n    \nexcept Exception as e:\n    print(f\"Skipping BERT embeddings due to error: {str(e)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport numpy as np\nimport pandas as pd\n\ndef create_ensemble_predictor(models, X_val, y_val):\n    \"\"\"\n    Create an ensemble predictor that combines multiple models with proper handling of NaN values\n    and feature name consistency\n    \n    Parameters:\n    models (dict): Dictionary of trained models\n    X_val (DataFrame): Validation features\n    y_val (Series): Validation targets\n    \n    Returns:\n    function: Ensemble prediction function\n    dict: Model weights\n    \"\"\"\n    print(f\"Creating ensemble predictor with {len(models)} models\")\n    print(f\"X_val shape: {X_val.shape}, NaN values: {X_val.isna().sum().sum()}\")\n    \n    # Ensure we have the imputer and scaler\n    imputer = models.get('imputer')\n    scaler = models.get('scaler')\n    \n    if imputer is None:\n        print(\"Warning: Imputer not found in models dictionary\")\n        # Create a simple imputer if none exists\n        from sklearn.impute import SimpleImputer\n        imputer = SimpleImputer(strategy='mean')\n        imputer.fit(X_val)\n        models['imputer'] = imputer\n    \n    # Get predictions from all available models\n    predictions = {}\n    \n    # Handle NaN values in validation data\n    X_val_imputed = pd.DataFrame(\n        imputer.transform(X_val), \n        columns=X_val.columns,\n        index=X_val.index\n    )\n    \n    print(f\"Imputed X_val shape: {X_val_imputed.shape}, NaN values: {X_val_imputed.isna().sum().sum()}\")\n    \n    # Make predictions with each model\n    if 'ridge' in models and models['ridge'] is not None:\n        try:\n            # Convert to numpy array to avoid feature name issues\n            X_scaled = scaler.transform(X_val_imputed.values)\n            ridge_preds = models['ridge'].predict(X_scaled)\n            predictions['ridge'] = np.round(np.clip(ridge_preds, 1, 10)).astype(int)\n            print(\"Successfully made Ridge predictions\")\n        except Exception as e:\n            print(f\"Error with Ridge model: {str(e)}\")\n    \n    if 'rf' in models and models['rf'] is not None:\n        try:\n            rf_preds = models['rf'].predict(X_val_imputed)\n            predictions['rf'] = np.round(np.clip(rf_preds, 1, 10)).astype(int)\n            print(\"Successfully made Random Forest predictions\")\n        except Exception as e:\n            print(f\"Error with Random Forest model: {str(e)}\")\n    \n    if 'gb' in models and models['gb'] is not None:\n        try:\n            gb_preds = models['gb'].predict(X_val_imputed)\n            predictions['gb'] = np.round(np.clip(gb_preds, 1, 10)).astype(int)\n            print(\"Successfully made Gradient Boosting predictions\")\n        except Exception as e:\n            print(f\"Error with Gradient Boosting model: {str(e)}\")\n    \n    if 'xgb' in models and models['xgb'] is not None:\n        try:\n            xgb_preds = models['xgb'].predict(X_val_imputed)\n            predictions['xgb'] = np.round(np.clip(xgb_preds, 1, 10)).astype(int)\n            print(\"Successfully made XGBoost predictions\")\n        except Exception as e:\n            print(f\"Error with XGBoost model: {str(e)}\")\n    \n    # If no models could make predictions, use a fallback\n    if not predictions:\n        print(\"No models could make predictions. Using default predictor.\")\n        # Return a function that always predicts middle score (5)\n        return (lambda x, **kwargs: 5), {'default': 1.0}\n    \n    # Optimize model weights based on MSE\n    weights = {}\n    for model_name, preds in predictions.items():\n        mse = mean_squared_error(y_val, preds)\n        print(f\"{model_name.upper()} MSE: {mse:.4f}\")\n        # Use inverse MSE as weight (better models get higher weights)\n        weights[model_name] = 1 / mse if mse > 0 else 1\n    \n    # Normalize weights\n    total_weight = sum(weights.values())\n    for model_name in weights:\n        weights[model_name] /= total_weight\n    \n    print(\"Ensemble model weights:\")\n    for model_name, weight in sorted(weights.items(), key=lambda x: x[1], reverse=True):\n        print(f\"{model_name}: {weight:.4f}\")\n    \n    # Create ensemble prediction function\n    def predict_ensemble(features, **kwargs):\n        \"\"\"\n        Make well-being score predictions using ensemble\n        \n        Parameters:\n        features (DataFrame): Features for traditional models\n        **kwargs: Additional arguments (ignored)\n        \n        Returns:\n        int: Predicted well-being score (1-10)\n        \"\"\"\n        # Ensure we're predicting for a single sample\n        if len(features.shape) > 1 and features.shape[0] > 1:\n            print(f\"Warning: Predicting for first sample only. Received {features.shape[0]} samples.\")\n            if isinstance(features, pd.DataFrame):\n                features = features.iloc[[0]]\n            else:\n                features = features[[0], :]\n        \n        # Preprocess: handle NaN values\n        try:\n            if isinstance(features, pd.DataFrame):\n                features_imputed = pd.DataFrame(\n                    imputer.transform(features),\n                    columns=features.columns,\n                    index=features.index\n                )\n            else:\n                features_imputed = imputer.transform(features)\n        except Exception as e:\n            print(f\"Error in imputation: {str(e)}\")\n            return 5  # Default middle score if preprocessing fails\n        \n        # Collect predictions from each model\n        model_predictions = []\n        model_weights = []\n        \n        # Ridge model (needs scaling)\n        if 'ridge' in weights:\n            try:\n                X_scaled = scaler.transform(features_imputed.values if isinstance(features_imputed, pd.DataFrame) \n                                          else features_imputed)\n                ridge_pred = models['ridge'].predict(X_scaled)[0]\n                model_predictions.append(int(np.round(np.clip(ridge_pred, 1, 10))))\n                model_weights.append(weights['ridge'])\n            except Exception as e:\n                print(f\"Ridge prediction error: {str(e)}\")\n        \n        # Random Forest model\n        if 'rf' in weights:\n            try:\n                rf_pred = models['rf'].predict(features_imputed)[0]\n                model_predictions.append(int(np.round(np.clip(rf_pred, 1, 10))))\n                model_weights.append(weights['rf'])\n            except Exception as e:\n                print(f\"Random Forest prediction error: {str(e)}\")\n        \n        # Gradient Boosting model\n        if 'gb' in weights:\n            try:\n                gb_pred = models['gb'].predict(features_imputed)[0]\n                model_predictions.append(int(np.round(np.clip(gb_pred, 1, 10))))\n                model_weights.append(weights['gb'])\n            except Exception as e:\n                print(f\"Gradient Boosting prediction error: {str(e)}\")\n        \n        # XGBoost model\n        if 'xgb' in weights:\n            try:\n                xgb_pred = models['xgb'].predict(features_imputed)[0]\n                model_predictions.append(int(np.round(np.clip(xgb_pred, 1, 10))))\n                model_weights.append(weights['xgb'])\n            except Exception as e:\n                print(f\"XGBoost prediction error: {str(e)}\")\n        \n        # If we have predictions, compute weighted average\n        if model_predictions:\n            # Weighted average\n            weighted_pred = sum(p * w for p, w in zip(model_predictions, model_weights)) / sum(model_weights)\n            final_pred = int(np.round(np.clip(weighted_pred, 1, 10)))\n            return final_pred\n        else:\n            # If all models failed, return middle score\n            print(\"All models failed to predict, returning default score\")\n            return 5\n    \n    return predict_ensemble, weights\n\n# Create ensemble predictor with error handling\ntry:\n    print(\"\\n=== Creating ensemble predictor ===\")\n    # First ensure that validation data is aligned with targets\n    if isinstance(val_target, pd.Series):\n        X_val_aligned = val_features_full.loc[val_target.index]\n    else:\n        X_val_aligned = val_features_full\n        \n    # Now create the ensemble predictor\n    ensemble_predictor, model_weights = create_ensemble_predictor(\n        models, X_val_aligned, val_target\n    )\n    print(\"Successfully created ensemble predictor\")\nexcept Exception as e:\n    print(f\"Failed to create ensemble predictor: {str(e)}\")\n    # Fallback to best model\n    if 'best_model' in models and models['best_model'] is not None:\n        print(\"Using best model as fallback\")\n        best_model = models['best_model']\n        imputer = models.get('imputer')\n        scaler = models.get('scaler')\n        \n        def simple_predictor(features, **kwargs):\n            try:\n                # Basic preprocessing\n                if imputer is not None:\n                    features_imputed = imputer.transform(features)\n                else:\n                    features_imputed = features\n                \n                # Check if best model is Ridge (needs scaling)\n                if hasattr(best_model, 'intercept_') and scaler is not None:\n                    features_scaled = scaler.transform(features_imputed)\n                    pred = best_model.predict(features_scaled)[0]\n                else:\n                    pred = best_model.predict(features_imputed)[0]\n                \n                return int(np.round(np.clip(pred, 1, 10)))\n            except Exception as e:\n                print(f\"Error in simple predictor: {str(e)}\")\n                return 5  # Default middle score\n        \n        ensemble_predictor = simple_predictor\n        model_weights = {'best_model': 1.0}\n    else:\n        print(\"No best model available, using default predictor\")\n        # Just return middle score\n        ensemble_predictor = lambda x, **kwargs: 5\n        model_weights = {'default': 1.0}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_wellbeing_for_post(post_text, previous_posts, adaptive_evidence, maladaptive_evidence, \n                              models, vectorizer, ensemble_predictor=None):\n    \"\"\"\n    Predict well-being score for a new post\n    \n    Parameters:\n    post_text (str): Text content of the post\n    previous_posts (list): List of previous post texts\n    adaptive_evidence (list): Extracted adaptive evidence spans\n    maladaptive_evidence (list): Extracted maladaptive evidence spans\n    models (dict): Trained models\n    vectorizer (TfidfVectorizer): Fitted vectorizer\n    ensemble_predictor (function, optional): Ensemble prediction function\n    \n    Returns:\n    int: Predicted well-being score (1-10)\n    \"\"\"\n    # Create a DataFrame with the post\n    post_df = pd.DataFrame([{\n        'text': post_text,\n        'adaptive_evidence': adaptive_evidence,\n        'maladaptive_evidence': maladaptive_evidence,\n        'previous_posts': previous_posts\n    }])\n    \n    # Extract features\n    features = extract_wellbeing_features(post_df)\n    \n    # Add text vectorization features\n    tfidf = vectorizer.transform([post_text])\n    tfidf_df = pd.DataFrame(\n        tfidf.toarray(), \n        columns=[f'tfidf_{i}' for i in range(tfidf.shape[1])]\n    )\n    \n    # Combine all features\n    features_full = pd.concat([features, tfidf_df], axis=1)\n    \n    # If ensemble predictor is available, use it\n    if ensemble_predictor is not None:\n        return ensemble_predictor(features_full)\n    \n    # Otherwise, use the best model\n    best_model = models['best_model']\n    prediction = best_model.predict(features_full)[0]\n    \n    # Round to nearest integer and clip to 1-10 range\n    final_prediction = int(np.round(np.clip(prediction, 1, 10)))\n    \n    return final_prediction\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_timeline_for_taskA(timeline, adaptive_clf, maladaptive_clf, vectorizer, feature_names,\n                              wellbeing_models, adaptive_importance=None, maladaptive_importance=None,\n                              ensemble_predictor=None):\n    \"\"\"\n    Process a test timeline for Task A (both A1 and A2)\n    \n    Parameters:\n    timeline (dict): Timeline data\n    adaptive_clf, maladaptive_clf: Classifiers from Task A1\n    vectorizer: TF-IDF vectorizer\n    feature_names: Feature names for the vectorizer\n    wellbeing_models (dict): Trained well-being prediction models\n    adaptive_importance, maladaptive_importance: Feature importance dictionaries\n    ensemble_predictor: Ensemble prediction function\n    \n    Returns:\n    tuple: (timeline_id, result dictionary)\n    \"\"\"\n    timeline_id = timeline[\"timeline_id\"]\n    result = {\n        \"timeline_level\": {\"summary\": \"\"},  # Will be filled by Task C\n        \"post_level\": {}\n    }\n    \n    # Keep track of previous posts for context\n    previous_posts = []\n    \n    for post in timeline[\"posts\"]:\n        post_id = post[\"post_id\"]\n        post_text = post[\"post\"] if \"post\" in post else \"\"\n        \n        if not post_text:\n            # Handle empty posts\n            result[\"post_level\"][post_id] = {\n                \"adaptive_evidence\": [],\n                \"maladaptive_evidence\": [],\n                \"summary\": \"\",\n                \"wellbeing_score\": None\n            }\n            continue\n        \n        # Task A.1: Extract adaptive and maladaptive evidence\n        adaptive_spans = extract_evidence_spans(\n            post_text, adaptive_clf, vectorizer, feature_names,\n            feature_importance=adaptive_importance\n        )\n        adaptive_spans = consolidate_spans(adaptive_spans, post_text)\n        \n        maladaptive_spans = extract_evidence_spans(\n            post_text, maladaptive_clf, vectorizer, feature_names,\n            feature_importance=maladaptive_importance\n        )\n        maladaptive_spans = consolidate_spans(maladaptive_spans, post_text)\n        \n        # Task A.2: Predict well-being score\n        wellbeing_score = predict_wellbeing_for_post(\n            post_text, previous_posts, adaptive_spans, maladaptive_spans,\n            wellbeing_models, vectorizer, ensemble_predictor\n        )\n        \n        # Add to results\n        result[\"post_level\"][post_id] = {\n            \"adaptive_evidence\": adaptive_spans,\n            \"maladaptive_evidence\": maladaptive_spans,\n            \"summary\": \"\",  # Will be filled by Task B\n            \"wellbeing_score\": wellbeing_score\n        }\n        \n        # Update previous posts for context\n        previous_posts.append(post_text)\n        if len(previous_posts) > 5:  # Keep only last 5 posts for context\n            previous_posts = previous_posts[-5:]\n    \n    return timeline_id, result\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\ndef run_full_taskA_pipeline(train_dir, test_dir, output_path, team_name=\"MyTeam\"):\n    \"\"\"Run the complete Task A pipeline (A1 and A2) from training to submission generation\"\"\"\n    print(\"Loading training data...\")\n    train_timelines = load_all_timelines(train_dir)\n    \n    # Task A.1 data preparation\n    train_df_a1 = create_training_dataset(train_timelines)\n    train_df_a1 = analyze_dataset(train_df_a1)\n    \n    # Task A.2 data preparation\n    wellbeing_df = extract_wellbeing_data(train_timelines)\n    \n    print(\"Splitting data...\")\n    # Split data for both tasks\n    train_data_a1, val_data_a1 = train_test_split(train_df_a1, test_size=0.2, random_state=42)\n    train_data_a2, val_data_a2 = train_test_split(wellbeing_df, test_size=0.2, random_state=42)\n    \n    print(\"Training Task A.1 classifiers...\")\n    # Task A.1: Feature engineering and model training\n    X_train_a1, vectorizer, feature_names = engineer_features(train_data_a1)\n    X_val_a1 = vectorizer.transform(val_data_a1['text'])\n    \n    adaptive_clf, maladaptive_clf = train_binary_classifiers(\n        X_train_a1, train_data_a1['has_adaptive'], train_data_a1['has_maladaptive'],\n        X_val_a1, val_data_a1['has_adaptive'], val_data_a1['has_maladaptive']\n    )\n    \n    # Calculate feature importance for Task A.1\n    adaptive_importance = calculate_feature_importance_simple(adaptive_clf, feature_names)\n    maladaptive_importance = calculate_feature_importance_simple(maladaptive_clf, feature_names)\n    \n    print(\"Training Task A.2 models...\")\n    # Task A.2: Feature engineering and model training\n    train_features_a2 = extract_wellbeing_features(train_data_a2)\n    val_features_a2 = extract_wellbeing_features(val_data_a2)\n    \n    # Add text vectorization features\n    train_tfidf_a2 = vectorizer.transform(train_data_a2['text'])\n    val_tfidf_a2 = vectorizer.transform(val_data_a2['text'])\n    \n    # Ensure indices are preserved during DataFrame creation\n    train_tfidf_df_a2 = pd.DataFrame(\n        train_tfidf_a2.toarray(), \n        columns=[f'tfidf_{i}' for i in range(train_tfidf_a2.shape[1])],\n        index=train_data_a2.index  # Preserve index alignment\n    )\n    val_tfidf_df_a2 = pd.DataFrame(\n        val_tfidf_a2.toarray(), \n        columns=[f'tfidf_{i}' for i in range(val_tfidf_a2.shape[1])],\n        index=val_data_a2.index  # Preserve index alignment\n    )\n    \n    # Ensure train_features_a2 and val_features_a2 have proper indices\n    train_features_a2.index = train_data_a2.index\n    val_features_a2.index = val_data_a2.index\n    \n    # Concatenate features with preserved indices\n    train_features_full_a2 = pd.concat([train_features_a2, train_tfidf_df_a2], axis=1)\n    val_features_full_a2 = pd.concat([val_features_a2, val_tfidf_df_a2], axis=1)\n    \n    # Ensure targets are Series with preserved indices\n    train_target_a2 = pd.Series(train_data_a2['wellbeing_score'].values, index=train_data_a2.index)\n    val_target_a2 = pd.Series(val_data_a2['wellbeing_score'].values, index=val_data_a2.index)\n    \n    # Train well-being models with properly aligned data\n    wellbeing_models = train_wellbeing_models(\n        train_features_full_a2, train_target_a2,\n        val_features_full_a2, val_target_a2\n    )\n    \n    print(\"Creating ensemble predictor...\")\n    # Create ensemble predictor using aligned data\n    ensemble_predictor, model_weights = create_aligned_ensemble_predictor(\n        wellbeing_models, val_features_full_a2, val_target_a2\n    )\n    \n    print(\"Processing test data...\")\n    test_timelines = load_all_timelines(test_dir)\n    \n    submission = {}\n    for timeline in tqdm(test_timelines, desc=\"Processing test timelines\"):\n        timeline_id, result = process_timeline_for_taskA(\n            timeline, adaptive_clf, maladaptive_clf, vectorizer, feature_names,\n            wellbeing_models, adaptive_importance, maladaptive_importance,\n            ensemble_predictor\n        )\n        submission[timeline_id] = result\n    \n    print(\"Saving submission...\")\n    os.makedirs(output_path, exist_ok=True)\n    output_file = os.path.join(output_path, f\"{team_name}_TaskA.json\")\n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(submission, f, ensure_ascii=False, indent=2)\n    \n    print(f\"Task A processing complete! Submission saved to {output_file}\")\n    return submission\n\ndef create_aligned_ensemble_predictor(models, X_val, y_val):\n    \"\"\"\n    Create an ensemble predictor that combines multiple models with proper data alignment\n    \n    Parameters:\n    models (dict): Dictionary of trained models\n    X_val (DataFrame): Validation features\n    y_val (Series): Validation targets\n    \n    Returns:\n    function: Ensemble prediction function\n    dict: Model weights\n    \"\"\"\n    print(f\"Creating ensemble predictor with {len(models)} models\")\n    \n    # Ensure X_val and y_val are properly aligned by index\n    common_indices = X_val.index.intersection(y_val.index)\n    print(f\"Original X_val shape: {X_val.shape}, y_val shape: {len(y_val)}\")\n    print(f\"Common indices: {len(common_indices)}\")\n    \n    # Filter both dataframes to only include common indices\n    X_val_aligned = X_val.loc[common_indices]\n    y_val_aligned = y_val.loc[common_indices]\n    \n    print(f\"Aligned X_val shape: {X_val_aligned.shape}, y_val shape: {len(y_val_aligned)}\")\n    \n    # Ensure we have the imputer and scaler\n    imputer = models.get('imputer')\n    scaler = models.get('scaler')\n    \n    if imputer is None:\n        print(\"Warning: Imputer not found in models dictionary\")\n        from sklearn.impute import SimpleImputer\n        imputer = SimpleImputer(strategy='mean')\n        imputer.fit(X_val_aligned)\n        models['imputer'] = imputer\n    \n    # Handle NaN values in validation data\n    X_val_imputed = pd.DataFrame(\n        imputer.transform(X_val_aligned), \n        columns=X_val_aligned.columns,\n        index=X_val_aligned.index\n    )\n    \n    print(f\"Imputed X_val shape: {X_val_imputed.shape}, NaN values: {X_val_imputed.isna().sum().sum()}\")\n    \n    # Get predictions from all available models\n    predictions = {}\n    \n    # Make predictions with each model\n    if 'ridge' in models and models['ridge'] is not None:\n        try:\n            # Convert to numpy array to avoid feature name issues\n            X_scaled = scaler.transform(X_val_imputed.values)\n            ridge_preds = models['ridge'].predict(X_scaled)\n            predictions['ridge'] = np.round(np.clip(ridge_preds, 1, 10)).astype(int)\n            print(\"Successfully made Ridge predictions\")\n        except Exception as e:\n            print(f\"Error with Ridge model: {str(e)}\")\n    \n    if 'rf' in models and models['rf'] is not None:\n        try:\n            # Convert to numpy array to avoid feature name issues\n            rf_preds = models['rf'].predict(X_val_imputed.values)\n            predictions['rf'] = np.round(np.clip(rf_preds, 1, 10)).astype(int)\n            print(\"Successfully made Random Forest predictions\")\n        except Exception as e:\n            print(f\"Error with Random Forest model: {str(e)}\")\n    \n    if 'gb' in models and models['gb'] is not None:\n        try:\n            # Convert to numpy array to avoid feature name issues\n            gb_preds = models['gb'].predict(X_val_imputed.values)\n            predictions['gb'] = np.round(np.clip(gb_preds, 1, 10)).astype(int)\n            print(\"Successfully made Gradient Boosting predictions\")\n        except Exception as e:\n            print(f\"Error with Gradient Boosting model: {str(e)}\")\n    \n    if 'xgb' in models and models['xgb'] is not None:\n        try:\n            # Convert to numpy array to avoid feature name issues\n            xgb_preds = models['xgb'].predict(X_val_imputed.values)\n            predictions['xgb'] = np.round(np.clip(xgb_preds, 1, 10)).astype(int)\n            print(\"Successfully made XGBoost predictions\")\n        except Exception as e:\n            print(f\"Error with XGBoost model: {str(e)}\")\n    \n    # If no models could make predictions, use a fallback\n    if not predictions:\n        print(\"No models could make predictions. Using default predictor.\")\n        return (lambda x, **kwargs: 5), {'default': 1.0}\n    \n    # Optimize model weights based on MSE\n    weights = {}\n    for model_name, preds in predictions.items():\n        # Now y_val_aligned and preds should have the same length\n        mse = mean_squared_error(y_val_aligned, preds)\n        print(f\"{model_name.upper()} MSE: {mse:.4f}\")\n        # Use inverse MSE as weight (better models get higher weights)\n        weights[model_name] = 1 / mse if mse > 0 else 1\n    \n    # Normalize weights\n    total_weight = sum(weights.values())\n    for model_name in weights:\n        weights[model_name] /= total_weight\n    \n    print(\"Ensemble model weights:\")\n    for model_name, weight in sorted(weights.items(), key=lambda x: x[1], reverse=True):\n        print(f\"{model_name}: {weight:.4f}\")\n    \n    # Create ensemble prediction function\n    def predict_ensemble(features, **kwargs):\n        \"\"\"\n        Make well-being score predictions using ensemble\n        \n        Parameters:\n        features (DataFrame): Features for traditional models\n        **kwargs: Additional arguments (ignored)\n        \n        Returns:\n        int: Predicted well-being score (1-10)\n        \"\"\"\n        # Ensure we're predicting for a single sample\n        if len(features.shape) > 1 and features.shape[0] > 1:\n            print(f\"Warning: Predicting for first sample only. Received {features.shape[0]} samples.\")\n            if isinstance(features, pd.DataFrame):\n                features = features.iloc[[0]]\n            else:\n                features = features[[0], :]\n        \n        # Preprocess: handle NaN values\n        try:\n            if isinstance(features, pd.DataFrame):\n                features_imputed = pd.DataFrame(\n                    imputer.transform(features),\n                    columns=features.columns,\n                    index=features.index\n                )\n            else:\n                features_imputed = imputer.transform(features)\n        except Exception as e:\n            print(f\"Error in imputation: {str(e)}\")\n            return 5  # Default middle score if preprocessing fails\n        \n        # Collect predictions from each model\n        model_predictions = []\n        model_weights = []\n        \n        # Ridge model (needs scaling)\n        if 'ridge' in weights:\n            try:\n                # Convert to numpy array to avoid feature name issues\n                X_scaled = scaler.transform(features_imputed.values if isinstance(features_imputed, pd.DataFrame) \n                                          else features_imputed)\n                ridge_pred = models['ridge'].predict(X_scaled)[0]\n                model_predictions.append(int(np.round(np.clip(ridge_pred, 1, 10))))\n                model_weights.append(weights['ridge'])\n            except Exception as e:\n                print(f\"Ridge prediction error: {str(e)}\")\n        \n        # Random Forest model\n        if 'rf' in weights:\n            try:\n                # Convert to numpy array to avoid feature name issues\n                rf_pred = models['rf'].predict(\n                    features_imputed.values if isinstance(features_imputed, pd.DataFrame) \n                    else features_imputed\n                )[0]\n                model_predictions.append(int(np.round(np.clip(rf_pred, 1, 10))))\n                model_weights.append(weights['rf'])\n            except Exception as e:\n                print(f\"Random Forest prediction error: {str(e)}\")\n        \n        # Gradient Boosting model\n        if 'gb' in weights:\n            try:\n                # Convert to numpy array to avoid feature name issues\n                gb_pred = models['gb'].predict(\n                    features_imputed.values if isinstance(features_imputed, pd.DataFrame) \n                    else features_imputed\n                )[0]\n                model_predictions.append(int(np.round(np.clip(gb_pred, 1, 10))))\n                model_weights.append(weights['gb'])\n            except Exception as e:\n                print(f\"Gradient Boosting prediction error: {str(e)}\")\n        \n        # XGBoost model\n        if 'xgb' in weights:\n            try:\n                # Convert to numpy array to avoid feature name issues\n                xgb_pred = models['xgb'].predict(\n                    features_imputed.values if isinstance(features_imputed, pd.DataFrame) \n                    else features_imputed\n                )[0]\n                model_predictions.append(int(np.round(np.clip(xgb_pred, 1, 10))))\n                model_weights.append(weights['xgb'])\n            except Exception as e:\n                print(f\"XGBoost prediction error: {str(e)}\")\n        \n        # If we have predictions, compute weighted average\n        if model_predictions:\n            # Weighted average\n            weighted_pred = sum(p * w for p, w in zip(model_predictions, model_weights)) / sum(model_weights)\n            final_pred = int(np.round(np.clip(weighted_pred, 1, 10)))\n            return final_pred\n        else:\n            # If all models failed, return middle score\n            print(\"All models failed to predict, returning default score\")\n            return 5\n    \n    return predict_ensemble, weights\n\n# Main execution\nif __name__ == \"__main__\":\n    # Configuration\n    TRAIN_DIR = \"/kaggle/input/train-dataset-1\"\n    TEST_DIR = \"/kaggle/input/test-dataset-1\"\n    OUTPUT_DIR = \"/kaggle/working/\"\n    TEAM_NAME = \"CIOL\"\n    \n    # Run the pipeline\n    submission = run_full_taskA_pipeline(TRAIN_DIR, TEST_DIR, OUTPUT_DIR, TEAM_NAME)\n    \n    print(\"Task A completed successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# For systems without GPU, we can use CPU but generation will be slower\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_examples_from_training_data(train_dir, num_examples=3):\n    \"\"\"\n    Load example post summaries from training data to use as few-shot examples\n    \"\"\"\n    examples = []\n    files = [f for f in os.listdir(train_dir) if f.endswith('.json')]\n    \n    for filename in files:\n        with open(os.path.join(train_dir, filename), 'r', encoding='utf-8') as f:\n            timeline = json.load(f)\n            \n        for post in timeline[\"posts\"]:\n            # Skip posts without summaries or evidence\n            if \"Post Summary\" not in post or not post[\"Post Summary\"]:\n                continue\n                \n            if \"evidence\" not in post:\n                continue\n                \n            adaptive_evidence = []\n            maladaptive_evidence = []\n            \n            # Extract evidence spans\n            if \"adaptive-state\" in post[\"evidence\"]:\n                for component, details in post[\"evidence\"][\"adaptive-state\"].items():\n                    if \"highlighted_evidence\" in details:\n                        adaptive_evidence.append(details[\"highlighted_evidence\"])\n            \n            if \"maladaptive-state\" in post[\"evidence\"]:\n                for component, details in post[\"evidence\"][\"maladaptive-state\"].items():\n                    if \"highlighted_evidence\" in details:\n                        maladaptive_evidence.append(details[\"highlighted_evidence\"])\n            \n            if not adaptive_evidence and not maladaptive_evidence:\n                continue\n                \n            wellbeing_score = post.get(\"Well-being\")\n            if wellbeing_score is None:\n                continue\n                \n            examples.append({\n                \"post\": post[\"post\"],\n                \"adaptive_evidence\": adaptive_evidence,\n                \"maladaptive_evidence\": maladaptive_evidence,\n                \"wellbeing_score\": wellbeing_score,\n                \"summary\": post[\"Post Summary\"]\n            })\n            \n            if len(examples) >= num_examples:\n                return examples\n    \n    return examples\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_evidence_list(evidence_list):\n    \"\"\"Format evidence list for prompt\"\"\"\n    if not evidence_list:\n        return \"None found.\"\n    \n    return \"\\n\".join([f\"- \\\"{item}\\\"\" for item in evidence_list])\n\ndef determine_dominant_state(adaptive_evidence, maladaptive_evidence, wellbeing_score):\n    \"\"\"Determine which self-state is dominant based on evidence and wellbeing score\"\"\"\n    has_adaptive = len(adaptive_evidence) > 0\n    has_maladaptive = len(maladaptive_evidence) > 0\n    \n    # Determine dominant state based on wellbeing score and evidence presence\n    if wellbeing_score >= 7 and has_adaptive:\n        return \"adaptive\"\n    elif wellbeing_score <= 5 and has_maladaptive:\n        return \"maladaptive\"\n    elif has_adaptive and not has_maladaptive:\n        return \"adaptive\"\n    elif has_maladaptive and not has_adaptive:\n        return \"maladaptive\"\n    elif len(adaptive_evidence) > len(maladaptive_evidence):\n        return \"adaptive\"\n    elif len(maladaptive_evidence) > len(adaptive_evidence):\n        return \"maladaptive\"\n    else:\n        # Default to maladaptive if unclear\n        return \"maladaptive\" if has_maladaptive else \"adaptive\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_summary_prompt_with_examples(post_text, adaptive_evidence, maladaptive_evidence, wellbeing_score, examples):\n    \"\"\"Create a prompt for the language model with few-shot examples\"\"\"\n    dominant_state = determine_dominant_state(adaptive_evidence, maladaptive_evidence, wellbeing_score)\n    \n    # Base prompt with instructions\n    base_prompt = f\"\"\"As a clinical psychologist expert in mental health, I need to generate a summary of self-states from a social media post. This summary should capture the interplay between adaptive (positive, constructive) and maladaptive (negative, harmful) self-states.\n\nSelf-states are characterized by specific combinations of Affect (A), Behavior (B), Cognition (C), and Desire/Need (D) that are co-activated for periods of time:\n- Affect (A): Emotions expressed (pride, anxiety, depression, etc.)\n- Behavior (B): Actions toward self (self-care, self-harm) or others (relating, controlling)\n- Cognition (C): Perceptions of self (self-acceptance, self-criticism) or others (related, detached)\n- Desire/Need/Expectation (D): What the person wants or expects\n\nThe summary should:\n1. Start with the dominant self-state and identify its central organizing aspect (A, B, C, or D)\n2. Describe how this central aspect influences other components\n3. Emphasize causal relationships between components\n4. Address the second self-state if present\n5. Follow the same structure for the second self-state\n\nThe summary should read naturally without explicitly labeling components as A, B, C, or D, but should clearly describe the psychological dynamics.\n\nHere are some examples:\"\"\"\n    \n    # Add few-shot examples\n    example_section = \"\"\n    for example in examples:\n        example_section += f\"\"\"\n\nExample Post:\n\"{example['post']}\"\n\nAdaptive Evidence:\n{format_evidence_list(example['adaptive_evidence'])}\n\nMaladaptive Evidence:\n{format_evidence_list(example['maladaptive_evidence'])}\n\nWell-being Score: {example['wellbeing_score']}\n\nSummary:\n{example['summary']}\n\"\"\"\n    \n    # Now add the current post information\n    current_post_section = f\"\"\"\n\nNow, please generate a summary for this post:\n\nPost:\n\"{post_text}\"\n\nAdaptive Evidence:\n{format_evidence_list(adaptive_evidence)}\n\nMaladaptive Evidence:\n{format_evidence_list(maladaptive_evidence)}\n\nWell-being Score: {wellbeing_score}\n\nBased on this information, the dominant self-state appears to be {dominant_state}.\n\nSummary:\"\"\"\n    \n    return base_prompt + example_section + current_post_section\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def post_process_summary(summary):\n    \"\"\"Clean up and improve the generated summary\"\"\"\n    # Remove any instruction residue or repetitions\n    summary = re.sub(r\"Summary:|The summary is:\", \"\", summary).strip()\n    \n    # Remove any \"In conclusion\" or similar phrases that might appear\n    summary = re.sub(r\"^In (summary|conclusion|this post),\", \"\", summary).strip()\n    \n    # Ensure the summary has proper paragraph structure\n    if \".\" in summary and not summary.endswith(\".\"):\n        # Get the last complete sentence\n        last_period = summary.rindex(\".\")\n        summary = summary[:last_period+1]\n    \n    # Ensure the summary isn't too short\n    if len(summary.split()) < 30:\n        return \"The post does not contain sufficient information to generate a meaningful summary of self-states.\"\n    \n    return summary\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_model_for_summarization(model_name=\"meta-llama/Llama-2-7b-chat-hf\"):\n    \"\"\"Load the language model for summarization\"\"\"\n    print(f\"Loading language model: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    # Add padding token if it doesn't exist\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    # Use 8-bit quantization to reduce memory requirements\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name, \n        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n        device_map=\"auto\",\n        load_in_8bit=True  # Use 8-bit quantization if available\n    )\n    return model, tokenizer\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_post_summary(post_text, adaptive_evidence, maladaptive_evidence, wellbeing_score, model, tokenizer, examples=None, max_length=500):\n    \"\"\"Generate a post-level summary using a language model approach\"\"\"\n    # Create a prompt for the language model\n    prompt = create_summary_prompt_with_examples(\n        post_text, adaptive_evidence, maladaptive_evidence, wellbeing_score, examples\n    )\n    \n    # Generate the summary\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Check if inputs are too long for the model\n    if inputs.input_ids.shape[1] > tokenizer.model_max_length:\n        print(f\"Warning: Input too long ({inputs.input_ids.shape[1]} tokens). Truncating.\")\n        inputs = tokenizer(\n            prompt, \n            truncation=True, \n            max_length=tokenizer.model_max_length - 100,  # Leave room for generation\n            return_tensors=\"pt\"\n        ).to(model.device)\n    \n    # Generate summary\n    try:\n        with torch.no_grad():\n            output = model.generate(\n                inputs.input_ids,\n                max_new_tokens=max_length,\n                num_return_sequences=1,\n                temperature=0.7,\n                top_p=0.9,\n                no_repeat_ngram_size=3,\n                do_sample=True\n            )\n        \n        # Decode the generated text\n        summary = tokenizer.decode(output[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n        \n        # Clean up the summary\n        summary = post_process_summary(summary)\n        \n        return summary\n    except Exception as e:\n        print(f\"Error generating summary: {str(e)}\")\n        return create_fallback_summary(post_text, adaptive_evidence, maladaptive_evidence, wellbeing_score)\ndef generate_post_summary(post_text, adaptive_evidence, maladaptive_evidence, wellbeing_score, model, tokenizer, examples=None, max_length=500):\n    \"\"\"Generate a post-level summary using a language model approach\"\"\"\n    # Create a prompt for the language model\n    prompt = create_summary_prompt_with_examples(\n        post_text, adaptive_evidence, maladaptive_evidence, wellbeing_score, examples\n    )\n    \n    # Generate the summary\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Check if inputs are too long for the model\n    if inputs.input_ids.shape[1] > tokenizer.model_max_length:\n        print(f\"Warning: Input too long ({inputs.input_ids.shape[1]} tokens). Truncating.\")\n        inputs = tokenizer(\n            prompt, \n            truncation=True, \n            max_length=tokenizer.model_max_length - 100,  # Leave room for generation\n            return_tensors=\"pt\"\n        ).to(model.device)\n    \n    # Generate summary\n    try:\n        with torch.no_grad():\n            output = model.generate(\n                inputs.input_ids,\n                max_new_tokens=max_length,\n                num_return_sequences=1,\n                temperature=0.7,\n                top_p=0.9,\n                no_repeat_ngram_size=3,\n                do_sample=True\n            )\n        \n        # Decode the generated text\n        summary = tokenizer.decode(output[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n        \n        # Clean up the summary\n        summary = post_process_summary(summary)\n        \n        return summary\n    except Exception as e:\n        print(f\"Error generating summary: {str(e)}\")\n        return create_fallback_summary(post_text, adaptive_evidence, maladaptive_evidence, wellbeing_score)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_fallback_summary(post_text, adaptive_evidence, maladaptive_evidence, wellbeing_score):\n    \"\"\"Create a simple template-based summary as a fallback\"\"\"\n    has_adaptive = len(adaptive_evidence) > 0\n    has_maladaptive = len(maladaptive_evidence) > 0\n    \n    # Determine dominant state\n    dominant_state = determine_dominant_state(adaptive_evidence, maladaptive_evidence, wellbeing_score)\n    \n    if dominant_state == \"adaptive\" and has_adaptive:\n        summary = f\"The dominant self-state in this post is adaptive. The writer shows evidence of positive emotional states and constructive behaviors as seen in '{adaptive_evidence[0]}'. \"\n        \n        if has_maladaptive:\n            summary += f\"However, there is also evidence of a maladaptive self-state as shown by '{maladaptive_evidence[0]}'. \"\n    elif dominant_state == \"maladaptive\" and has_maladaptive:\n        summary = f\"The dominant self-state in this post is maladaptive. The writer shows evidence of negative emotional states and harmful patterns as seen in '{maladaptive_evidence[0]}'. \"\n        \n        if has_adaptive:\n            summary += f\"However, there is also evidence of an adaptive self-state as shown by '{adaptive_evidence[0]}'. \"\n    else:\n        summary = \"The post shows insufficient evidence to determine a clear dominant self-state.\"\n    \n    return summary\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_posts_in_batches(taskA_output, test_timelines, model, tokenizer, examples, batch_size=8):\n    \"\"\"Process posts in batches for better efficiency\"\"\"\n    # Create a mapping of timeline_id, post_id to post_text\n    post_text_map = {}\n    for timeline in test_timelines:\n        timeline_id = timeline[\"timeline_id\"]\n        for post in timeline[\"posts\"]:\n            post_id = post[\"post_id\"]\n            post_text = post[\"post\"]\n            post_text_map[(timeline_id, post_id)] = post_text\n    \n    # Create a flat list of all posts to process\n    posts_to_process = []\n    for timeline_id, timeline_data in taskA_output.items():\n        for post_id, post_data in timeline_data[\"post_level\"].items():\n            # Get data from Task A\n            adaptive_evidence = post_data.get(\"adaptive_evidence\", [])\n            maladaptive_evidence = post_data.get(\"maladaptive_evidence\", [])\n            wellbeing_score = post_data.get(\"wellbeing_score\")\n            \n            # Skip posts without evidence or wellbeing score\n            if (not adaptive_evidence and not maladaptive_evidence) or wellbeing_score is None:\n                continue\n            \n            # Get the post text from our mapping\n            post_text = post_text_map.get((timeline_id, post_id), \"\")\n            \n            if not post_text:\n                print(f\"Warning: Could not find text for post {post_id} in timeline {timeline_id}\")\n                continue\n            \n            posts_to_process.append({\n                \"timeline_id\": timeline_id,\n                \"post_id\": post_id,\n                \"post_text\": post_text,\n                \"adaptive_evidence\": adaptive_evidence,\n                \"maladaptive_evidence\": maladaptive_evidence,\n                \"wellbeing_score\": wellbeing_score\n            })\n    \n    # Process posts in batches\n    taskB_output = taskA_output.copy()\n    total_posts = len(posts_to_process)\n    print(f\"Processing {total_posts} posts in batches of {batch_size}\")\n    \n    for i in range(0, total_posts, batch_size):\n        batch = posts_to_process[i:i+batch_size]\n        print(f\"Processing batch {i//batch_size + 1}/{(total_posts + batch_size - 1)//batch_size}\")\n        \n        for post_data in tqdm(batch, desc=\"Generating summaries\"):\n            timeline_id = post_data[\"timeline_id\"]\n            post_id = post_data[\"post_id\"]\n            \n            # Generate summary\n            summary = generate_post_summary(\n                post_data[\"post_text\"],\n                post_data[\"adaptive_evidence\"],\n                post_data[\"maladaptive_evidence\"],\n                post_data[\"wellbeing_score\"],\n                model,\n                tokenizer,\n                examples=examples\n            )\n            \n            # Add summary to output\n            taskB_output[timeline_id][\"post_level\"][post_id][\"summary\"] = summary\n    \n    # Ensure all posts have summaries (even empty ones for posts without evidence)\n    for timeline_id, timeline_data in taskA_output.items():\n        for post_id in timeline_data[\"post_level\"]:\n            if \"summary\" not in taskB_output[timeline_id][\"post_level\"][post_id]:\n                taskB_output[timeline_id][\"post_level\"][post_id][\"summary\"] = \"\"\n    \n    return taskB_output\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_task_b_pipeline(taskA_output, test_timelines, train_dir, model_name=\"meta-llama/Llama-2-7b-chat-hf\"):\n    \"\"\"Run the full Task B pipeline\"\"\"\n    # Load examples from training data\n    print(\"Loading examples from training data...\")\n    examples = load_examples_from_training_data(train_dir, num_examples=3)\n    \n    # Load the language model\n    model, tokenizer = load_model_for_summarization(model_name)\n    \n    # Process posts in batches\n    print(\"Processing posts...\")\n    taskB_output = process_posts_in_batches(\n        taskA_output, test_timelines, model, tokenizer, examples\n    )\n    \n    return taskB_output\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install huggingface_hub\nfrom huggingface_hub import notebook_login\n\n# Log in to your Hugging Face account\nnotebook_login()\n\n\ndef main(train_dir, test_dir, taskA_output_path, output_path, model_name=\"meta-llama/Llama-2-7b-chat-hf\"):\n    \"\"\"Main function to run Task B pipeline\"\"\"\n    # Load Task A output\n    print(f\"Loading Task A output from {taskA_output_path}\")\n    with open(taskA_output_path, 'r', encoding='utf-8') as f:\n        taskA_output = json.load(f)\n    \n    # Load test timelines\n    print(f\"Loading test timelines from {test_dir}\")\n    test_timelines = []\n    for filename in os.listdir(test_dir):\n        if filename.endswith('.json'):\n            with open(os.path.join(test_dir, filename), 'r', encoding='utf-8') as f:\n                test_timelines.append(json.load(f))\n    \n    # Run Task B pipeline\n    print(\"Running Task B pipeline...\")\n    taskB_output = run_task_b_pipeline(taskA_output, test_timelines, train_dir, model_name)\n    \n    # Save output\n    print(f\"Saving output to {output_path}\")\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump(taskB_output, f, ensure_ascii=False, indent=2)\n    \n    print(\"Task B processing complete!\")\n    return taskB_output\n\ndef load_model_for_summarization(model_name=\"meta-llama/Llama-2-7b-chat-hf\"):\n    \"\"\"Load the language model for summarization\"\"\"\n    print(f\"Loading language model: {model_name}\")\n    \n    # Use the access token to load the model\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True) \n    \n    # ... (rest of the function) ...\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n        device_map=\"auto\",\n        load_in_8bit=True,  # Use 8-bit quantization if available\n        use_auth_token=True  # Use the access token to load the model\n    )\n    return model, tokenizer\n\nif __name__ == \"__main__\":\n    # Instead of using argparse, directly call main with your desired values:\n    train_dir = \"/kaggle/input/train-dataset-1\" # Replace with your actual path\n    test_dir = \"/kaggle/input/test-dataset-1\"     # Replace with your actual path\n    taskA_output_path = \"/kaggle/working/CIOL_TaskA.json\" # Replace with your actual path\n    output_path = \"/kaggle/working/\" # Replace with your actual path\n    model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # You can change this if needed\n    \n    main(train_dir, test_dir, taskA_output_path, output_path, model_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}